{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XMzmpgphzMU"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/tulasiram58827/ocr_tflite/blob/main/colabs/KERAS_OCR_TFLITE.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml8RTXFZxPkd"
      },
      "source": [
        "## SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4o-1GNYVxWUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a79fcfb-14c8-4f09-b8da-0a373d06ab24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from validators) (1.15.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators) (4.4.2)\n",
            "Installing collected packages: validators\n",
            "Successfully installed validators-0.18.2\n"
          ]
        }
      ],
      "source": [
        "!pip install validators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5Mme4MUCxVM8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9007a843-56da-4c79-bb2c-e5894cb6d75a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import typing\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwgbqcnBxnc9"
      },
      "source": [
        "### Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "duqhKKqbxpc9"
      },
      "outputs": [],
      "source": [
        "DEFAULT_BUILD_PARAMS = {\n",
        "    'height': 31,\n",
        "    'width': 200,\n",
        "    'color': False,\n",
        "    'filters': (64, 128, 256, 256, 512, 512, 512),\n",
        "    'rnn_units': (128, 128),\n",
        "    'dropout': 0.25,\n",
        "    'rnn_steps_to_discard': 2,\n",
        "    'pool_size': 2,\n",
        "    'stn': True,\n",
        "}\n",
        "\n",
        "DEFAULT_ALPHABET = string.digits + string.ascii_lowercase\n",
        "\n",
        "PRETRAINED_WEIGHTS = {\n",
        "    'kurapan': {\n",
        "        'alphabet': DEFAULT_ALPHABET,\n",
        "        'build_params': DEFAULT_BUILD_PARAMS,\n",
        "        'weights': {\n",
        "            'notop': {\n",
        "                'url':\n",
        "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan_notop.h5',\n",
        "                'filename': 'crnn_kurapan_notop.h5',\n",
        "                'sha256': '027fd2cced3cbea0c4f5894bb8e9e85bac04f11daf96b8fdcf1e4ee95dcf51b9'\n",
        "            },\n",
        "            'top': {\n",
        "                'url':\n",
        "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan.h5',\n",
        "                'filename': 'crnn_kurapan.h5',\n",
        "                'sha256': 'a7d8086ac8f5c3d6a0a828f7d6fbabcaf815415dd125c32533013f85603be46d'\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH9oviLCzOI6"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GkP9-8YZzTWh"
      },
      "outputs": [],
      "source": [
        "def swish(x, beta=1):\n",
        "    return x * keras.backend.sigmoid(beta * x)\n",
        "\n",
        "\n",
        "keras.utils.get_custom_objects().update({'swish': keras.layers.Activation(swish)})\n",
        "\n",
        "\n",
        "def _repeat(x, num_repeats):\n",
        "    ones = tf.ones((1, num_repeats), dtype='int32')\n",
        "    x = tf.reshape(x, shape=(-1, 1))\n",
        "    x = tf.matmul(x, ones)\n",
        "    return tf.reshape(x, [-1])\n",
        "\n",
        "\n",
        "def _meshgrid(height, width):\n",
        "    x_linspace = tf.linspace(-1., 1., width)\n",
        "    y_linspace = tf.linspace(-1., 1., height)\n",
        "    x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
        "    x_coordinates = tf.reshape(x_coordinates, shape=(1, -1))\n",
        "    y_coordinates = tf.reshape(y_coordinates, shape=(1, -1))\n",
        "    ones = tf.ones_like(x_coordinates)\n",
        "    indices_grid = tf.concat([x_coordinates, y_coordinates, ones], 0)\n",
        "    return indices_grid\n",
        "\n",
        "\n",
        "# pylint: disable=too-many-statements\n",
        "def _transform(inputs):\n",
        "    locnet_x, locnet_y = inputs\n",
        "    output_size = locnet_x.shape[1:]\n",
        "    batch_size = tf.shape(locnet_x)[0]\n",
        "    height = tf.shape(locnet_x)[1]\n",
        "    width = tf.shape(locnet_x)[2]\n",
        "    num_channels = tf.shape(locnet_x)[3]\n",
        "\n",
        "    locnet_y = tf.reshape(locnet_y, shape=(batch_size, 2, 3))\n",
        "\n",
        "    locnet_y = tf.reshape(locnet_y, (-1, 2, 3))\n",
        "    locnet_y = tf.cast(locnet_y, 'float32')\n",
        "\n",
        "    output_height = output_size[0]\n",
        "    output_width = output_size[1]\n",
        "    indices_grid = _meshgrid(output_height, output_width)\n",
        "    indices_grid = tf.expand_dims(indices_grid, 0)\n",
        "    indices_grid = tf.reshape(indices_grid, [-1])  # flatten?\n",
        "    indices_grid = tf.tile(indices_grid, tf.stack([batch_size]))\n",
        "    indices_grid = tf.reshape(indices_grid, tf.stack([batch_size, 3, -1]))\n",
        "\n",
        "    transformed_grid = tf.matmul(locnet_y, indices_grid)\n",
        "    x_s = tf.slice(transformed_grid, [0, 0, 0], [-1, 1, -1])\n",
        "    y_s = tf.slice(transformed_grid, [0, 1, 0], [-1, 1, -1])\n",
        "    x = tf.reshape(x_s, [-1])\n",
        "    y = tf.reshape(y_s, [-1])\n",
        "\n",
        "    # Interpolate\n",
        "    height_float = tf.cast(height, dtype='float32')\n",
        "    width_float = tf.cast(width, dtype='float32')\n",
        "\n",
        "    output_height = output_size[0]\n",
        "    output_width = output_size[1]\n",
        "\n",
        "    x = tf.cast(x, dtype='float32')\n",
        "    y = tf.cast(y, dtype='float32')\n",
        "    x = .5 * (x + 1.0) * width_float\n",
        "    y = .5 * (y + 1.0) * height_float\n",
        "\n",
        "    x0 = tf.cast(tf.floor(x), 'int32')\n",
        "    x1 = x0 + 1\n",
        "    y0 = tf.cast(tf.floor(y), 'int32')\n",
        "    y1 = y0 + 1\n",
        "\n",
        "    max_y = tf.cast(height - 1, dtype='int32')\n",
        "    max_x = tf.cast(width - 1, dtype='int32')\n",
        "    zero = tf.zeros([], dtype='int32')\n",
        "\n",
        "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
        "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
        "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
        "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
        "\n",
        "    flat_image_dimensions = width * height\n",
        "    pixels_batch = tf.range(batch_size) * flat_image_dimensions\n",
        "    flat_output_dimensions = output_height * output_width\n",
        "    base = _repeat(pixels_batch, flat_output_dimensions)\n",
        "    base_y0 = base + y0 * width\n",
        "    base_y1 = base + y1 * width\n",
        "    indices_a = base_y0 + x0\n",
        "    indices_b = base_y1 + x0\n",
        "    indices_c = base_y0 + x1\n",
        "    indices_d = base_y1 + x1\n",
        "\n",
        "    flat_image = tf.reshape(locnet_x, shape=(-1, num_channels))\n",
        "    flat_image = tf.cast(flat_image, dtype='float32')\n",
        "    pixel_values_a = tf.gather(flat_image, indices_a)\n",
        "    pixel_values_b = tf.gather(flat_image, indices_b)\n",
        "    pixel_values_c = tf.gather(flat_image, indices_c)\n",
        "    pixel_values_d = tf.gather(flat_image, indices_d)\n",
        "\n",
        "    x0 = tf.cast(x0, 'float32')\n",
        "    x1 = tf.cast(x1, 'float32')\n",
        "    y0 = tf.cast(y0, 'float32')\n",
        "    y1 = tf.cast(y1, 'float32')\n",
        "\n",
        "    area_a = tf.expand_dims(((x1 - x) * (y1 - y)), 1)\n",
        "    area_b = tf.expand_dims(((x1 - x) * (y - y0)), 1)\n",
        "    area_c = tf.expand_dims(((x - x0) * (y1 - y)), 1)\n",
        "    area_d = tf.expand_dims(((x - x0) * (y - y0)), 1)\n",
        "    transformed_image = tf.add_n([\n",
        "        area_a * pixel_values_a, area_b * pixel_values_b, area_c * pixel_values_c,\n",
        "        area_d * pixel_values_d\n",
        "    ])\n",
        "    # Finished interpolation\n",
        "\n",
        "    transformed_image = tf.reshape(transformed_image,\n",
        "                                   shape=(batch_size, output_height, output_width, num_channels))\n",
        "    return transformed_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALRK3SR1x8JH"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dgDwWfpUhzM_"
      },
      "outputs": [],
      "source": [
        "def CTCDecoder():\n",
        "    def decoder(y_pred):\n",
        "        input_shape = tf.keras.backend.shape(y_pred)\n",
        "        input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(\n",
        "            input_shape[1], 'float32')\n",
        "        unpadded = tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]\n",
        "        unpadded_shape = tf.keras.backend.shape(unpadded)\n",
        "        padded = tf.pad(unpadded,\n",
        "                        paddings=[[0, 0], [0, input_shape[1] - unpadded_shape[1]]],\n",
        "                        constant_values=-1)\n",
        "        return padded\n",
        "\n",
        "    return tf.keras.layers.Lambda(decoder, name='decode')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DaRONZgExrQL"
      },
      "outputs": [],
      "source": [
        "def build_model(alphabet,\n",
        "                height,\n",
        "                width,\n",
        "                color,\n",
        "                filters,\n",
        "                rnn_units,\n",
        "                dropout,\n",
        "                rnn_steps_to_discard,\n",
        "                pool_size,\n",
        "                stn=True):\n",
        "    \"\"\"Build a Keras CRNN model for character recognition.\n",
        "    Args:\n",
        "        height: The height of cropped images\n",
        "        width: The width of cropped images\n",
        "        color: Whether the inputs should be in color (RGB)\n",
        "        filters: The number of filters to use for each of the 7 convolutional layers\n",
        "        rnn_units: The number of units for each of the RNN layers\n",
        "        dropout: The dropout to use for the final layer\n",
        "        rnn_steps_to_discard: The number of initial RNN steps to discard\n",
        "        pool_size: The size of the pooling steps\n",
        "        stn: Whether to add a Spatial Transformer layer\n",
        "    \"\"\"\n",
        "    assert len(filters) == 7, '7 CNN filters must be provided.'\n",
        "    assert len(rnn_units) == 2, '2 RNN filters must be provided.'\n",
        "    inputs = keras.layers.Input((height, width, 3 if color else 1), name='input', batch_size=1)\n",
        "    x = keras.layers.Permute((2, 1, 3))(inputs)\n",
        "    x = keras.layers.Lambda(lambda x: x[:, :, ::-1])(x)\n",
        "    x = keras.layers.Conv2D(filters[0], (3, 3), activation='relu', padding='same', name='conv_1')(x)\n",
        "    x = keras.layers.Conv2D(filters[1], (3, 3), activation='relu', padding='same', name='conv_2')(x)\n",
        "    x = keras.layers.Conv2D(filters[2], (3, 3), activation='relu', padding='same', name='conv_3')(x)\n",
        "    x = keras.layers.BatchNormalization(name='bn_3')(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_3')(x)\n",
        "    x = keras.layers.Conv2D(filters[3], (3, 3), activation='relu', padding='same', name='conv_4')(x)\n",
        "    x = keras.layers.Conv2D(filters[4], (3, 3), activation='relu', padding='same', name='conv_5')(x)\n",
        "    x = keras.layers.BatchNormalization(name='bn_5')(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_5')(x)\n",
        "    x = keras.layers.Conv2D(filters[5], (3, 3), activation='relu', padding='same', name='conv_6')(x)\n",
        "    x = keras.layers.Conv2D(filters[6], (3, 3), activation='relu', padding='same', name='conv_7')(x)\n",
        "    x = keras.layers.BatchNormalization(name='bn_7')(x)\n",
        "    if stn:\n",
        "        # pylint: disable=pointless-string-statement\n",
        "        \"\"\"Spatial Transformer Layer\n",
        "        Implements a spatial transformer layer as described in [1]_.\n",
        "        Borrowed from [2]_:\n",
        "        downsample_fator : float\n",
        "            A value of 1 will keep the orignal size of the image.\n",
        "            Values larger than 1 will down sample the image. Values below 1 will\n",
        "            upsample the image.\n",
        "            example image: height= 100, width = 200\n",
        "            downsample_factor = 2\n",
        "            output image will then be 50, 100\n",
        "        References\n",
        "        ----------\n",
        "        .. [1]  Spatial Transformer Networks\n",
        "                Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n",
        "                Submitted on 5 Jun 2015\n",
        "        .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n",
        "        .. [3]  https://github.com/EderSantana/seya/blob/keras1/seya/layers/attention.py\n",
        "        \"\"\"\n",
        "        stn_input_output_shape = (width // pool_size**2, height // pool_size**2, filters[6])\n",
        "        stn_input_layer = keras.layers.Input(shape=stn_input_output_shape)\n",
        "        locnet_y = keras.layers.Conv2D(16, (5, 5), padding='same',\n",
        "                                       activation='relu')(stn_input_layer)\n",
        "        locnet_y = keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu')(locnet_y)\n",
        "        locnet_y = keras.layers.Flatten()(locnet_y)\n",
        "        locnet_y = keras.layers.Dense(64, activation='relu')(locnet_y)\n",
        "        locnet_y = keras.layers.Dense(6,\n",
        "                                      weights=[\n",
        "                                          np.zeros((64, 6), dtype='float32'),\n",
        "                                          np.float32([[1, 0, 0], [0, 1, 0]]).flatten()\n",
        "                                      ])(locnet_y)\n",
        "        localization_net = keras.models.Model(inputs=stn_input_layer, outputs=locnet_y)\n",
        "        x = keras.layers.Lambda(_transform,\n",
        "                                output_shape=stn_input_output_shape)([x, localization_net(x)])\n",
        "    x = keras.layers.Reshape(target_shape=(width // pool_size**2,\n",
        "                                           (height // pool_size**2) * filters[-1]),\n",
        "                             name='reshape')(x)\n",
        "\n",
        "    x = keras.layers.Dense(rnn_units[0], activation='relu', name='fc_9')(x)\n",
        "\n",
        "    rnn_1_forward = keras.layers.LSTM(rnn_units[0],\n",
        "                                      kernel_initializer=\"he_normal\",\n",
        "                                      return_sequences=True,\n",
        "                                      name='lstm_10')(x)\n",
        "    rnn_1_back = keras.layers.LSTM(rnn_units[0],\n",
        "                                   kernel_initializer=\"he_normal\",\n",
        "                                   go_backwards=True,\n",
        "                                   return_sequences=True,\n",
        "                                   name='lstm_10_back')(x)\n",
        "    rnn_1_add = keras.layers.Add()([rnn_1_forward, rnn_1_back])\n",
        "    rnn_2_forward = keras.layers.LSTM(rnn_units[1],\n",
        "                                      kernel_initializer=\"he_normal\",\n",
        "                                      return_sequences=True,\n",
        "                                      name='lstm_11')(rnn_1_add)\n",
        "    rnn_2_back = keras.layers.LSTM(rnn_units[1],\n",
        "                                   kernel_initializer=\"he_normal\",\n",
        "                                   go_backwards=True,\n",
        "                                   return_sequences=True,\n",
        "                                   name='lstm_11_back')(rnn_1_add)\n",
        "    x = keras.layers.Concatenate()([rnn_2_forward, rnn_2_back])\n",
        "    backbone = keras.models.Model(inputs=inputs, outputs=x)\n",
        "    x = keras.layers.Dropout(dropout, name='dropout')(x)\n",
        "    x = keras.layers.Dense(len(alphabet) + 1,\n",
        "                           kernel_initializer='he_normal',\n",
        "                           activation='softmax',\n",
        "                           name='fc_12')(x)\n",
        "    x = keras.layers.Lambda(lambda x: x[:, rnn_steps_to_discard:])(x)\n",
        "    model = keras.models.Model(inputs=inputs, outputs=x)\n",
        "    prediction_model = keras.models.Model(inputs=inputs, outputs=CTCDecoder()(model.output))\n",
        "    return model, prediction_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NPBjgj6gyECQ"
      },
      "outputs": [],
      "source": [
        "build_params = DEFAULT_BUILD_PARAMS\n",
        "alphabets = DEFAULT_ALPHABET\n",
        "blank_index = len(alphabets)\n",
        "\n",
        "model, prediction_model = build_model(alphabet=alphabets, **build_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLVl95alzwI8"
      },
      "source": [
        "## Download and Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dmBRQeL90HoQ"
      },
      "outputs": [],
      "source": [
        "def get_default_cache_dir():\n",
        "    return os.environ.get('KERAS_OCR_CACHE_DIR', os.path.expanduser(os.path.join('~',\n",
        "                                                                              '.keras-ocr')))\n",
        "def sha256sum(filename):\n",
        "    \"\"\"Compute the sha256 hash for a file.\"\"\"\n",
        "    h = hashlib.sha256()\n",
        "    b = bytearray(128 * 1024)\n",
        "    mv = memoryview(b)\n",
        "    with open(filename, 'rb', buffering=0) as f:\n",
        "        for n in iter(lambda: f.readinto(mv), 0):\n",
        "            h.update(mv[:n])\n",
        "    return h.hexdigest()\n",
        "\n",
        "def download_and_verify(url, sha256=None, cache_dir=None, verbose=True, filename=None):\n",
        "    \"\"\"Download a file to a cache directory and verify it with a sha256\n",
        "    hash.\n",
        "    Args:\n",
        "        url: The file to download\n",
        "        sha256: The sha256 hash to check. If the file already exists and the hash\n",
        "            matches, we don't download it again.\n",
        "        cache_dir: The directory in which to cache the file. The default is\n",
        "            `~/.keras-ocr`.\n",
        "        verbose: Whether to log progress\n",
        "        filename: The filename to use for the file. By default, the filename is\n",
        "            derived from the URL.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = get_default_cache_dir()\n",
        "    if filename is None:\n",
        "        filename = os.path.basename(urllib.parse.urlparse(url).path)\n",
        "    filepath = os.path.join(cache_dir, filename)\n",
        "    os.makedirs(os.path.split(filepath)[0], exist_ok=True)\n",
        "    if verbose:\n",
        "        print('Looking for ' + filepath)\n",
        "    if not os.path.isfile(filepath) or (sha256 and sha256sum(filepath) != sha256):\n",
        "        if verbose:\n",
        "            print('Downloading ' + filepath)\n",
        "        urllib.request.urlretrieve(url, filepath)\n",
        "    assert sha256 is None or sha256 == sha256sum(filepath), 'Error occurred verifying sha256.'\n",
        "    return filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5lNpYwczLuO",
        "outputId": "03c7aa23-2a3d-47d3-d92a-72ef4e2bdbfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "Downloading /root/.keras-ocr/crnn_kurapan.h5\n"
          ]
        }
      ],
      "source": [
        "weights_dict = PRETRAINED_WEIGHTS['kurapan']\n",
        "\n",
        "model.load_weights(download_and_verify(url=weights_dict['weights']['top']['url'],\n",
        "                                       filename=weights_dict['weights']['top']['filename'],\n",
        "                                       sha256=weights_dict['weights']['top']['sha256']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czQXrbwF1rWO"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CUyx8EAV0IjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50de8e71-748a-4b9c-94e4-df72f839a60a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(1, 31, 200, 1)]    0           []                               \n",
            "                                                                                                  \n",
            " permute (Permute)              (1, 200, 31, 1)      0           ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (1, 200, 31, 1)      0           ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " conv_1 (Conv2D)                (1, 200, 31, 64)     640         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " conv_2 (Conv2D)                (1, 200, 31, 128)    73856       ['conv_1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv_3 (Conv2D)                (1, 200, 31, 256)    295168      ['conv_2[0][0]']                 \n",
            "                                                                                                  \n",
            " bn_3 (BatchNormalization)      (1, 200, 31, 256)    1024        ['conv_3[0][0]']                 \n",
            "                                                                                                  \n",
            " maxpool_3 (MaxPooling2D)       (1, 100, 15, 256)    0           ['bn_3[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_4 (Conv2D)                (1, 100, 15, 256)    590080      ['maxpool_3[0][0]']              \n",
            "                                                                                                  \n",
            " conv_5 (Conv2D)                (1, 100, 15, 512)    1180160     ['conv_4[0][0]']                 \n",
            "                                                                                                  \n",
            " bn_5 (BatchNormalization)      (1, 100, 15, 512)    2048        ['conv_5[0][0]']                 \n",
            "                                                                                                  \n",
            " maxpool_5 (MaxPooling2D)       (1, 50, 7, 512)      0           ['bn_5[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_6 (Conv2D)                (1, 50, 7, 512)      2359808     ['maxpool_5[0][0]']              \n",
            "                                                                                                  \n",
            " conv_7 (Conv2D)                (1, 50, 7, 512)      2359808     ['conv_6[0][0]']                 \n",
            "                                                                                                  \n",
            " bn_7 (BatchNormalization)      (1, 50, 7, 512)      2048        ['conv_7[0][0]']                 \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 6)            934902      ['bn_7[0][0]']                   \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (1, 50, 7, 512)      0           ['bn_7[0][0]',                   \n",
            "                                                                  'model[0][0]']                  \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (1, 50, 3584)        0           ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " fc_9 (Dense)                   (1, 50, 128)         458880      ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_10 (LSTM)                 (1, 50, 128)         131584      ['fc_9[0][0]']                   \n",
            "                                                                                                  \n",
            " lstm_10_back (LSTM)            (1, 50, 128)         131584      ['fc_9[0][0]']                   \n",
            "                                                                                                  \n",
            " add (Add)                      (1, 50, 128)         0           ['lstm_10[0][0]',                \n",
            "                                                                  'lstm_10_back[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_11 (LSTM)                 (1, 50, 128)         131584      ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " lstm_11_back (LSTM)            (1, 50, 128)         131584      ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (1, 50, 256)         0           ['lstm_11[0][0]',                \n",
            "                                                                  'lstm_11_back[0][0]']           \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (1, 50, 256)         0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " fc_12 (Dense)                  (1, 50, 37)          9509        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (1, 48, 37)          0           ['fc_12[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,794,267\n",
            "Trainable params: 8,791,707\n",
            "Non-trainable params: 2,560\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dg50iNc106w"
      },
      "source": [
        "## Convert to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y9sC2rpTe-bX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d08623-d858-40f4-b3b9-2e1efd25fc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  represent_data.zip\n",
            "   creating: represent_data/\n",
            "  inflating: represent_data/word_1.png  \n",
            "  inflating: represent_data/word_2.png  \n",
            "  inflating: represent_data/word_3.png  \n",
            "  inflating: represent_data/word_4.png  \n",
            "  inflating: represent_data/word_5.png  \n",
            "  inflating: represent_data/word_6.png  \n",
            "  inflating: represent_data/word_7.png  \n",
            "  inflating: represent_data/word_8.png  \n",
            "  inflating: represent_data/word_9.png  \n",
            "  inflating: represent_data/word_10.png  \n",
            "  inflating: represent_data/word_11.png  \n",
            "  inflating: represent_data/word_12.png  \n",
            "  inflating: represent_data/word_13.png  \n",
            "  inflating: represent_data/word_14.png  \n",
            "  inflating: represent_data/word_15.png  \n",
            "  inflating: represent_data/word_16.png  \n",
            "  inflating: represent_data/word_17.png  \n",
            "  inflating: represent_data/word_18.png  \n",
            "  inflating: represent_data/word_19.png  \n",
            "  inflating: represent_data/word_20.png  \n",
            "  inflating: represent_data/word_21.png  \n",
            "  inflating: represent_data/word_22.png  \n",
            "  inflating: represent_data/word_23.png  \n",
            "  inflating: represent_data/word_24.png  \n",
            "  inflating: represent_data/word_25.png  \n",
            "  inflating: represent_data/word_26.png  \n",
            "  inflating: represent_data/word_27.png  \n",
            "  inflating: represent_data/word_28.png  \n",
            "  inflating: represent_data/word_29.png  \n",
            "  inflating: represent_data/word_30.png  \n",
            "  inflating: represent_data/word_31.png  \n",
            "  inflating: represent_data/word_32.png  \n",
            "  inflating: represent_data/word_33.png  \n",
            "  inflating: represent_data/word_34.png  \n",
            "  inflating: represent_data/word_35.png  \n",
            "  inflating: represent_data/word_36.png  \n",
            "  inflating: represent_data/word_37.png  \n",
            "  inflating: represent_data/word_38.png  \n",
            "  inflating: represent_data/word_39.png  \n",
            "  inflating: represent_data/word_40.png  \n",
            "  inflating: represent_data/word_41.png  \n",
            "  inflating: represent_data/word_42.png  \n",
            "  inflating: represent_data/word_43.png  \n",
            "  inflating: represent_data/word_44.png  \n",
            "  inflating: represent_data/word_45.png  \n",
            "  inflating: represent_data/word_46.png  \n",
            "  inflating: represent_data/word_47.png  \n",
            "  inflating: represent_data/word_48.png  \n",
            "  inflating: represent_data/word_49.png  \n",
            "  inflating: represent_data/word_50.png  \n",
            "  inflating: represent_data/word_51.png  \n",
            "  inflating: represent_data/word_52.png  \n",
            "  inflating: represent_data/word_53.png  \n",
            "  inflating: represent_data/word_54.png  \n",
            "  inflating: represent_data/word_55.png  \n",
            "  inflating: represent_data/word_56.png  \n",
            "  inflating: represent_data/word_57.png  \n",
            "  inflating: represent_data/word_58.png  \n",
            "  inflating: represent_data/word_59.png  \n",
            "  inflating: represent_data/word_60.png  \n",
            "  inflating: represent_data/word_61.png  \n",
            "  inflating: represent_data/word_62.png  \n",
            "  inflating: represent_data/word_63.png  \n",
            "  inflating: represent_data/word_64.png  \n",
            "  inflating: represent_data/word_65.png  \n",
            "  inflating: represent_data/word_66.png  \n",
            "  inflating: represent_data/word_67.png  \n",
            "  inflating: represent_data/word_68.png  \n",
            "  inflating: represent_data/word_69.png  \n",
            "  inflating: represent_data/word_70.png  \n",
            "  inflating: represent_data/word_71.png  \n",
            "  inflating: represent_data/word_72.png  \n",
            "  inflating: represent_data/word_73.png  \n",
            "  inflating: represent_data/word_74.png  \n",
            "  inflating: represent_data/word_75.png  \n",
            "  inflating: represent_data/word_76.png  \n",
            "  inflating: represent_data/word_77.png  \n",
            "  inflating: represent_data/word_78.png  \n",
            "  inflating: represent_data/word_79.png  \n",
            "  inflating: represent_data/word_80.png  \n",
            "  inflating: represent_data/word_81.png  \n",
            "  inflating: represent_data/word_82.png  \n",
            "  inflating: represent_data/word_83.png  \n",
            "  inflating: represent_data/word_84.png  \n",
            "  inflating: represent_data/word_85.png  \n",
            "  inflating: represent_data/word_86.png  \n",
            "  inflating: represent_data/word_87.png  \n",
            "  inflating: represent_data/word_88.png  \n",
            "  inflating: represent_data/word_89.png  \n",
            "  inflating: represent_data/word_90.png  \n",
            "  inflating: represent_data/word_91.png  \n",
            "  inflating: represent_data/word_92.png  \n",
            "  inflating: represent_data/word_93.png  \n",
            "  inflating: represent_data/word_94.png  \n",
            "  inflating: represent_data/word_95.png  \n",
            "  inflating: represent_data/word_96.png  \n",
            "  inflating: represent_data/word_97.png  \n",
            "  inflating: represent_data/word_98.png  \n",
            "  inflating: represent_data/word_99.png  \n",
            "  inflating: represent_data/word_100.png  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2022-04-05 08:03:37--  https://github.com/tulasiram58827/ocr_tflite/raw/main/data/represent_data.zip\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tulasiram58827/ocr_tflite/main/data/represent_data.zip [following]\n",
            "--2022-04-05 08:03:37--  https://raw.githubusercontent.com/tulasiram58827/ocr_tflite/main/data/represent_data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 585568 (572K) [application/zip]\n",
            "Saving to: ‘represent_data.zip’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  8% 12.6M 0s\n",
            "    50K .......... .......... .......... .......... .......... 17% 11.0M 0s\n",
            "   100K .......... .......... .......... .......... .......... 26% 33.7M 0s\n",
            "   150K .......... .......... .......... .......... .......... 34% 65.1M 0s\n",
            "   200K .......... .......... .......... .......... .......... 43% 25.0M 0s\n",
            "   250K .......... .......... .......... .......... .......... 52% 26.2M 0s\n",
            "   300K .......... .......... .......... .......... .......... 61% 97.9M 0s\n",
            "   350K .......... .......... .......... .......... .......... 69%  370M 0s\n",
            "   400K .......... .......... .......... .......... .......... 78%  456M 0s\n",
            "   450K .......... .......... .......... .......... .......... 87% 27.9M 0s\n",
            "   500K .......... .......... .......... .......... .......... 96%  190M 0s\n",
            "   550K .......... .......... .                               100%  171M=0.02s\n",
            "\n",
            "2022-04-05 08:03:38 (32.4 MB/s) - ‘represent_data.zip’ saved [585568/585568]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download and unzipping representative dataset\n",
        "%%bash\n",
        "wget https://github.com/tulasiram58827/ocr_tflite/raw/main/data/represent_data.zip\n",
        "unzip represent_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-vYzKpI1fB27"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/represent_data/'\n",
        "def representative_data_gen():\n",
        "    for file in os.listdir(dataset_path):\n",
        "        image_path = dataset_path + file\n",
        "        input_data = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        input_data = cv2.resize(input_data, (200, 31))\n",
        "        input_data = input_data[np.newaxis]\n",
        "        input_data = np.expand_dims(input_data, 3)\n",
        "        input_data = input_data.astype('float32')/255\n",
        "        yield [input_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VzyIgV-NfEeb"
      },
      "outputs": [],
      "source": [
        "def convert_tflite(quantization):\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(prediction_model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "    if quantization == 'float16':\n",
        "        converter.target_spec.supported_types = [tf.float16]\n",
        "    elif quantization == 'int8' or quantization == 'full_int8':\n",
        "        converter.representative_dataset = representative_data_gen\n",
        "    if quantization == 'full_int8':\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "        converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "    tf_lite_model = converter.convert()\n",
        "    open(f'ocr_{quantization}.tflite', 'wb').write(tf_lite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN216EKBfLt7",
        "outputId": "cdd18e11-3fd8-4404-811e-cc621037e7b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp2rqa4nsw/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp2rqa4nsw/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "quantization = 'dr' #@param [\"dr\", \"float16\"]\n",
        "convert_tflite(quantization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhWmneGKfQpl",
        "outputId": "22442fe0-366a-43d2-94bf-029d3b1b741d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.5M\tocr_dr.tflite\n"
          ]
        }
      ],
      "source": [
        "!du -sh ocr_dr.tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2EWZDvpfQrY",
        "outputId": "d483a2ac-5623-4ba8-cfa9-4f9a2aa649ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpuq8idp09/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpuq8idp09/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "quantization = 'float16' #@param [\"dr\", \"float16\"]\n",
        "convert_tflite(quantization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqOjRGHmfYHX",
        "outputId": "51f73d78-f9e9-4b6e-a879-017e23258720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17M\tocr_float16.tflite\n"
          ]
        }
      ],
      "source": [
        "!du -sh ocr_float16.tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SohE_ShAfceP",
        "outputId": "eacba70c-b12f-4e19-bb52-23cdc0a2680a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpl5vgam0x/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpl5vgam0x/assets\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "quantization = 'int8'  #@param [\"dr\", \"float16\", 'int8', 'full_int8']\n",
        "convert_tflite(quantization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVhhiWe1fzBz"
      },
      "source": [
        "## TFLite Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rkGs7BDWf6dm"
      },
      "outputs": [],
      "source": [
        "def run_tflite_model(image_path, quantization):\n",
        "    input_data = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    input_data = cv2.resize(input_data, (200, 31))\n",
        "    input_data = input_data[np.newaxis]\n",
        "    input_data = np.expand_dims(input_data, 3)\n",
        "    input_data = input_data.astype('float32')/255\n",
        "    path = f'ocr_{quantization}.tflite'\n",
        "    interpreter = tf.lite.Interpreter(model_path=path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensors.\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    input_shape = input_details[0]['shape']\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "    interpreter.invoke()\n",
        "\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DGmtdkD7gAVH"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "image_path = '/content/represent_data/word_80.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "v6K9tv0u1umk",
        "outputId": "1c9d76fe-761f-4ae8-e25e-e904b84257f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=36x17 at 0x7FA320D83290>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACQAAAARCAIAAAA61ZnRAAAFNklEQVR4nH2US6yeZRHHZ+a5ve93/3ou9Fw4PVrSxpBawKqRRCsxSiVqYNHIQhes1Bh1a1y5MC50o9GFLIyVkBgjGhtDoihBJEgoGCjUtlartRxayzmn3/29PJcZF3wlpBJnNclM5jf/mcxg/MsaiFPMkNfjqktq1qQMU6q8oqZXIQKpIDFwbXSDxCIk4SmyA0qsMEQw2iFEDRJ9stZKJUGzOAdTUsZHVRowmEBQaQAAgImGRl22NJKKdSGUgc6VhElibaWhJM+U45AkKRRN2jABcK0YTLLElISRSFOLywKd0aKq5MkE1ii+QYilDRphDgPMFeAEvCudNkSRwZQQc9YUmZkrpUtRopWCaACYQQBBYUIqABhYojhExE69U9llM8LaOU1BtCUnsGtChqLnsFymEPzfL682yFZqV0e10O2s9GYCCYwk1F4yEXJklKmYx0wKsZ3YCkbApI0R5hQnzz63/8Wz7mufPZOrCMwhoja1ACqhKGkOY2YwrW8/um9cMTZuzWpZQDz+masfv+tSqktHhgBBxxR3mEyCLqInThSFSITLhFMBQ7p5dWyf/1vX6lsgJlBj7RwHAsySjiBhDlOqFmb29NC9W5+6Z9vP8Ie/2P+TZ/JP3mWRZmRtrHPFquaRtU6lHEOFOiQikS6j09kQk4DPkEKlApsSoagEPJmmiYlnriTIb+wscQeQvWLtdbtCwPCuxeLsVh8CBptfGHR//5QbbPfWlpr33D3eWLmiU+8/Y3r21fz0+V7e7By9Oz9067ANpeEOaJNSnUi9fGpl4NOH7py02gLWDP0ivQkjLBVEBfWr/85+/vTa95/YePyVxa98bNdb5Uv43sPvvrTb3zg4/Otu9o0TB8vpXuD0o99t/uYP+w5scoPhmyf2X3y9B6BTaBEEncwzLy//4GQXOqlvGDwFzUpdmyvT9bKYIailK7Pp5FKcTUMGqbLBRv/ia61Bqb917MLSQn3tcP9L313/8/nyjvdkp88tfeGBi8fee6Uq8zM/dn86ZfffP0h6SrDnt+cXfnryloeO73749rrUSnHfznZMI5vDYj7xgSFs33vHaw8cLQLPnnxp8+HH1m9fG71+2U4U9jbMmKVtpwCxGq3vvDEA75c6oQbH7WyhN5jutJtYNHFs0uTRkyurm/Gjh67mzlfB2hQ460mq52PUqSCVk2uE2ugZ5MhWD5M1o4nOW1QFPS18B6QqDDe9aU8zrI2tx0Xf+VmjHMXBame5xnTdQ7+KS1//3NndC3sfe24vTHKrIvA0SB25nCsrwUBiiKNXrizgC/nl6xvn/pmv9arbDszyTljB6c8eXz50ZHjmVLfN/s6D2/0urrbtL/8YXG91+197tq7hp+8b1ErX4NBdP7wpn/jI6V89tbnW5A8cHjZNw1EoqHXjqFEA7PvWi4r75/4RCyVHj/DxD16EMu7r9r/64LVHfr385Ln11Yy/eP/WqqJQ4ZcfPHPiibXvPLLhhO47tvX+taG2drM/PLJsxpE/f+yNWXRPv9S5baFsrEwjVaDa+OYjrnVqcPI+F1eZQOTSNOpmKpAaKVE0lWGOmhkMi3MhKGqzQCU1Km1QcYzW1XVFkYams2SvD0D3Btq3TUFlzuS1MEu8sTOQIoLVPmCdbBum0ELFWo0zUDAjCGBbUray0BJkpSRREdSlhgoGRiGNrSmLFJ3STb0YJ2PRbVCcJ6ZCk1IVtAT3VLiI/vkDUVUG59SbTETmjxrxpshb3ttDb+X/jyEAaKJRrhqJ+Z1T3lbnHZv4v4Cbm/sviNTcO7DJQYAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Running Dynamic Range Quantization\n",
        "tflite_output = run_tflite_model(image_path, 'dr')\n",
        "final_output = \"\".join(alphabets[index] for index in tflite_output[0] if index not in [blank_index, -1])\n",
        "print(final_output)\n",
        "cv2_imshow(cv2.imread(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DPsb744KgGqw",
        "outputId": "ee67c50c-6a0b-4eea-f309-671e706b8b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frons\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=39x18 at 0x7FA320815950>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACcAAAASCAIAAABXdlB8AAAG3klEQVR4nC3GWYxdZR0A8P+3n/Odc+46eymFmRFQKLYUDYsBWiIQNSQkBKSGGGOikYQW3ngwISwuuCaauGFM44NoXVBweTFKCQpYKARH2qFAt+kyc+fO3OVs3+6D/p5+aKqzCwAABRQAwKMAgAn8nwfAAB6BB/A4YAAAZAEA/e8AOHiKsEegaDAYAgISQFig3hPvAoDF4LBHGIj31INH2OMYU8EqqwhjAXkA5W1dlzUBgQIiiOnKUxTh4JEPVWkikQohjNHBI0bjRGYmgGFU0wDIluN+K4qCUtaVAZvaK0+ClKnVwWrDIqECKI+VMdgY670HjABZwaCR4naDEVSU41WrBpITjpHRNQR30UXbxuNyvbfBuUizttJ2Y7CZNLJAUF5XnNNOo1mNazCQysw5H8usVq6/PkpEmsbJaJS7QGSSJYnEGOMsawiCy/FQVwNKSqdOInd6etJNT+C6OFeMV2NGOOGnT57BiG/demmt/er6OouiyupK1QRBzLjTztdIjT2HlIWszrEqUSYnY96EwJ0lnCWJbOR5MRhsoEb2YWt1IlhDhnvvuvXOO/e02sloPBA8WVp69+tf+dG5s73Jmem80BAa7Xb32Ok3ZqemKJHD4ebkbKsqxkVRRFxYp7K4gXzibUDIYfBa15hxSrgLtqyGLvhIZDZYHgNNohg8D7YydZ7GsG3rxC8OHuytrzeb3eFmPRhu+OBRsBAsBpOPeguzM8aW+WZ/qtscrr3HOc8EJdiVuqLAer1eIqVMWFmO0ixZW1tJZIvxKIqA0MiaIHk0yvvU24BRgOCCqUkosVfPP//c8nvvKwVZmg7HeqIzzYTFZX7tzp2HD79a16MkZTuuWthz682vvHpoeXl5nDvtIZNi61zbFOWNH9vZbEW/e/aXZ865m27cdeWHrnlr6ejrR47ItBuLibVev9WVVCkVCx4zyoFGFJBXVVW4ADyCALrblV/44t477rjl7Nkz2z947U+f/vHB3xz47nce++g11/U3ep/77O1Hl48/9dT3j759fPuVlz/66GMnT5ydnZ2++JKJT+/d/dI/Dt2y+9Ytc/MXVjee/tnP//yXFyjrZmnstceUcBSCtcYa5WztbP3w/v1PPv74D3/wvXvuvisvBlqP0gZaPnbkvs/c+fs/PPPgg5+f29J96OEH9uz51N6997Wb2f59X4pjdP7c+9u2zpxbOXHfvXc/tO+B2ZnOHbftfuKJL+/YedPG5ur999/DWVg9f0ow5I3FAEApRQgZY6y1GMhVV1y144rtl29bnJucbsjIWy0oO3DgwIkTK2Wtrrvh5r8feuWf//qPcnD0ndUXXnxtcXFRREjKqCiKXz3za6vg5ZfeVBVePnbqzSMrwcOrr7zWbDaVqhjHCEEkBA3gPBDBOIKYspjQ6OGH9r/z7ikfFKVUK0dwtN4bDUY1o6LVnr504YrfPvvHCxv1pVvn+v3eWm8o04yLGBERRTEgRmiEMdEKWYMTyYWAqjT5uAYfMMZlmTMS4xBCAKecVy7UFrQXwyIUmo1r6nCDsAnjEuOkMZyy5ka/OP7OqU57qptGm+t9DL7T6ZSlN4ZVtXPAPZBSKecBMMUkNtoTHDEqE9mgRELAlPI4FpgwbKytldEeOSRp3BJJhydtFre0Z5XFzc5sd3pLUZlef+A8LC0tffy23Z/45O3jSl29/bJrdl39178d2hwZG7hyKGt0EOaBUEIFpmw4suNcIxJRIq2DwbhwAfKywIiSQDGLZdLoGi9WN6pBbWuMDaHAOc+yzWK8cuF8d2pyYqozGPa/8c0nj739xiOP7Dv88vM/efrbZbHx1a99q1R4Zsv82Qv9UpvK2DhJXfDjvJqc6k5OzOTj+tSplWaz3Wp24jghIkIT3V3aGgp+NFydv6izsLBweOm49gGBppSa0guOFy+bOfrvtxqybXRVVuvzl0wvbvvA/Pwla2unn/vTizKbq3QwKr/h+uuX3z6OQljtr1y+MC9ldOzou5QkWbOxsHjx66+9gWlc1S5KMzQ1+ZGAQpIk+bAHpiAIY5EoY0OwgvNEttbXVgnTnBKwIkkkwXVV5sNBOdHpalMFBJV2zXarrkYY09FGlcqMM0Qx1HXNWYSRULpkwiKKIpHWxgGJUau9o6rKVqMZnKLBhBACYc4FKaNinEdMAnhAxhhDSay1Hub9uZk5AlxrWxXjKIkCBQDnbY0CZkhiIEkS99YuCMEJIcF6IaO87CMC1vgkbXrMsPU+TTPkrDMV51xZgxBCCGEgUkSqHuKgMVAM1KGaxKHV6Q7yYqxGPEaMEaNdAKoNxHIiEk0AjBCq6zqSMRWoVEPZpJUeBuS5kO3O7HBYmKr+L5z2yQqFAR8hAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Running Float16 Quantization\n",
        "tflite_output = run_tflite_model(image_path, 'float16')\n",
        "final_output = \"\".join(alphabets[index] for index in tflite_output[0] if index not in [blank_index, -1])\n",
        "print(final_output)\n",
        "cv2_imshow(cv2.imread(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kyBwZRO3gPH6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "3bd24456-8633-4082-af1a-fe776167f4ac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-691fbd163b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running Integer Quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtflite_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_tflite_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphabets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblank_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'decoded' is not defined"
          ]
        }
      ],
      "source": [
        "# Running Integer Quantization\n",
        "tflite_output = run_tflite_model(image_path, 'int8')\n",
        "final_output = \"\".join(alphabets[index] for index in decoded[0] if index not in [blank_index, -1])\n",
        "print(final_output)\n",
        "cv2_imshow(cv2.imread(image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-4Ro3Z-gfjK"
      },
      "source": [
        "## Dynamic Range Model benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt5wQBSggfXg"
      },
      "source": [
        "**Inference Time** : 0.2sec\n",
        "\n",
        "**Memory FootPrint** : 46.38MB\n",
        "\n",
        "**Model Size** : 8.5MB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLqDwjRzgm_8"
      },
      "source": [
        "## Float16 benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1181AuvDgm1V"
      },
      "source": [
        "**Inference** : 0.76sec\n",
        "\n",
        "**Memory FootPrint** : 128MB\n",
        "\n",
        "**Model Size** : 17MB"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Using Keras _OCR_TFLITE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}